{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Approaches to the Study of Food Sign Patterns\n",
    "Today, we're going to explore the ways in which restaurants of different cuisines describe food on their menus. Specifically, we're interested in the following questions about food cultures in Chicago:\n",
    "1. Are there patterns in the ways in which particular cuisine genres describe food on menus?\n",
    "    * Identification of dicent indexical/iconic legisigns that position a particular restaurant within a cuisine (which can then be used by consumers to position themselves in the same light via social media posts, and so on)\n",
    "2. Are these cuisines (and/or menu discourse patterns identified in #1) geographically patterned?\n",
    "    * Identification of dicent indexical legisigns that point to a particular cuisine or broader menu discourse pattern on the basis of spatial location\n",
    "    \n",
    "First, let's load our packages. In order to run this notebook, you will need to install the `folium` package, which is available to install through the Anaconda Navigator or on the command line via the command `conda install -c conda-forge folium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jon Clindaniel\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "from gensim import corpora, models\n",
    "from sklearn.manifold import TSNE\n",
    "import folium\n",
    "\n",
    "# Some Functions from last time to get us started:\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "def get_lemmas(text):\n",
    "    # Combine list elements together into a single string for analysis\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    # Note the addition of some size-related stop words that are specific to Menu data and need to be removed\n",
    "    stop = nltk.corpus.stopwords.words('english') + list(string.punctuation) + ['comma', 'oz', '32', '32oz',\n",
    "                                                                                '16oz', '12', 'amp', 'w/', 'w.',\n",
    "                                                                                'z'\n",
    "                                                                               ]\n",
    "    tokens = [i for i in nltk.word_tokenize(text.lower()) if i not in stop]\n",
    "    lemmas = [nltk.stem.WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in tokens]\n",
    "    return lemmas\n",
    "\n",
    "def plot_top_tfidf(series, data_description):\n",
    "    # Apply 'get lemmas' function to any Pandas Series that we pass in to get lemmas for each row in the Series\n",
    "    lemmas = series.apply(get_lemmas)\n",
    "\n",
    "    # Initialize Series of lemmas as Gensim Dictionary for further processing\n",
    "    dictionary = corpora.Dictionary([i for i in lemmas])\n",
    "\n",
    "    # Convert dictionary into bag of words format: list of (token_id, token_count) tuples\n",
    "    bow_corpus = [dictionary.doc2bow(text) for text in lemmas]\n",
    "\n",
    "    # Calculate TFIDF based on bag of words counts for each token and return weights:\n",
    "    tfidf = models.TfidfModel(bow_corpus)\n",
    "    tfidf_weights=[]\n",
    "    \n",
    "    for doc in tfidf[bow_corpus]:\n",
    "        tfidf_weights.extend([[dictionary[ID], np.around(freq, decimals=2)] for ID, freq in doc])\n",
    "\n",
    "    # Sort TFIDF weights highest to lowest:\n",
    "    sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "    # Plot the top 10 weighted words:\n",
    "    top_10 = {k:v for k,v in sorted_tfidf_weights[:10]} # dictionary comprehension\n",
    "    plt.plot(list(top_10.keys()), list(top_10.values()), label=data_description)\n",
    "    plt.xticks(rotation='vertical')\n",
    "    plt.title('Top 10 Lemmas (TFIDF) for ' + data_description);\n",
    "\n",
    "    return\n",
    "\n",
    "# ... and some new functions that we'll work with today:\n",
    "\n",
    "def get_menu_data(url, uid, cuisine):\n",
    "    '''\n",
    "    Takes allmenus.com url and a restaurant unique id number\n",
    "    (so that restaurants with the same name are uniquely identifiable)\n",
    "    Returns Dictionary: {UID:{Cuisine, Restaurant Name,\n",
    "                              Coordinates, Menu:{Dish Names, Dish Descriptions}\n",
    "                        }\n",
    "    '''\n",
    "    # Get HTML, parse it, and identify JSON containing Restaurant/Menu Data\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "    restaurant_data = soup.find('script', type='application/ld+json')\n",
    "    try:\n",
    "        # set strict to False, so that control characters like \\n are allowed within the text:\n",
    "        restaurant_data_json = json.loads(restaurant_data.text, strict=False)\n",
    "    except:\n",
    "        return {uid: {'Cuisine': cuisine,\n",
    "                       'Restaurant': None,\n",
    "                       'Coordinates': None,\n",
    "                       'Menu Items': None,\n",
    "                       'Item Descriptions': None,\n",
    "                       'URL': url\n",
    "                }}\n",
    "    try:\n",
    "        # Store each menu item (a dictionary) into nested_items variable for retrieving item names and descriptions below:\n",
    "        nested_items = [section['hasMenuItem'] for section in restaurant_data_json['hasMenu'][0]['hasMenuSection']]\n",
    "\n",
    "        # Store Data for individual menu/restaurant in a dictionary to enable easy aggregation into a Pandas DataFrame:\n",
    "        menu_data = {uid: {'Cuisine': cuisine,\n",
    "                       'Restaurant': restaurant_data_json['name'],\n",
    "                       'Coordinates': (restaurant_data_json['geo']['latitude'], restaurant_data_json['geo']['longitude']),\n",
    "                       'Menu Items': [j['name'] for i in nested_items for j in i],\n",
    "                       'Item Descriptions': [j['description'] for i in nested_items for j in i],\n",
    "                       'URL': url\n",
    "                }}\n",
    "    except:\n",
    "        menu_data = {uid: {'Cuisine': cuisine,\n",
    "                       'Restaurant': restaurant_data_json['name'],\n",
    "                       'Coordinates': (restaurant_data_json['geo']['latitude'], restaurant_data_json['geo']['longitude']),\n",
    "                       'Menu Items': None,\n",
    "                       'Item Descriptions': None,\n",
    "                       'URL': url\n",
    "                }}\n",
    "\n",
    "    return menu_data\n",
    "\n",
    "def scrape_top_menus(cuisine_list, topn=10):\n",
    "    # Create a Dictionary for holding every cuisine's restaurant/menu data:\n",
    "    cuisine_dict = {}\n",
    "    # Incremental count for assigning unique identification numbers to each restaurant\n",
    "    count = 0\n",
    "\n",
    "    # Scrape top n (parameter topn) restaurant links from allmenus.com for each cuisine:\n",
    "    for cuisine in cuisine_list:\n",
    "        # Get cuisine's popularity-sorted list of restaurants\n",
    "        html = requests.get('https://www.allmenus.com/il/chicago/-/' + cuisine + '/')\n",
    "        soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "        # Find links where class != grubhub (these take you to Grubhub's website, rather than the allmenus page we want)\n",
    "        restaurant_anchors = soup.find('ul', class_=\"restaurant-list\").findAll(\"a\", class_=None)\n",
    "        restaurant_links = ['https://www.allmenus.com' + i.get('href') for i in restaurant_anchors[:topn]]\n",
    "\n",
    "        # Loop through each Restaurant's allmenus.com webpage and get its menu data:\n",
    "        menu_dict = {}\n",
    "        for restaurant in restaurant_links:\n",
    "            dict_temp = {}\n",
    "            dict_temp = get_menu_data(url = restaurant,\n",
    "                                         uid = count,\n",
    "                                         cuisine=cuisine)\n",
    "            menu_dict.update(dict_temp)\n",
    "            count += 1\n",
    "        cuisine_dict.update(menu_dict)\n",
    "\n",
    "\n",
    "    return pd.DataFrame(cuisine_dict).T\n",
    "\n",
    "def make_folium_pt_map(menu_df):\n",
    "    m = folium.Map(zoom_start=12, location=[41.8507, -87.6340], tiles='CartoDB positron')\n",
    "\n",
    "    for point in menu_df.index:\n",
    "        popup_content = '<b>Restaurant: </b>' + menu_df['Restaurant'][point] + '\\n' + '<b>Cuisine: </b>' + menu_df['Cuisine'][point].capitalize()\n",
    "        \n",
    "        # Colors are baked into the function, so we won't be able to reassign colors/cuisines without reorganizing:\n",
    "        color_cuisine = {'italian': 'red','soul-food': 'blue', 'latin-american': 'green', 'gastropub': 'purple',\n",
    "                         'polish': 'orange', 'late-night': 'darkred', 'thai': 'black', 'chinese': 'pink',\n",
    "                         'korean': 'darkblue', 'vegan_vegetarian':'lightgreen'\n",
    "                        }\n",
    "        color_icon_body = color_cuisine[menu_df['Cuisine'][point]]\n",
    "\n",
    "        folium.Marker(menu_df['Coordinates'][point],\n",
    "                      popup=popup_content,\n",
    "                      icon=folium.Icon(color=color_icon_body,icon_color='white', icon='cutlery')\n",
    "                     ).add_to(m)\n",
    "\n",
    "    # Legend Inspired by: https://medium.com/@bobhaffner/creating-a-legend-for-a-folium-map-c1e0ffc34373\n",
    "    legend_html = '''\n",
    "             <div style=\"position: fixed;\n",
    "             top: 50px; right: 50px; width: 150px; height: 325px;\n",
    "             border:2px solid grey; z-index:9999; font-size:14px;\n",
    "             \">&nbsp; Cuisine Color Code<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:red\"></i>&nbsp; Italian<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:blue\"></i>&nbsp; Soul Food<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:green\"></i>&nbsp; Latin American<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:purple\"></i>&nbsp; Gastropub<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:orange\"></i>&nbsp; Polish <br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:darkred\"></i>&nbsp; Late Night<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:black\"></i>&nbsp; Thai<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:pink\"></i>&nbsp; Chinese<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:darkblue\"></i>&nbsp; Korean<br>\n",
    "             &nbsp;<i class=\"fa fa-map-marker fa-2x\" style=\"color:lightgreen\"></i>&nbsp; Vegan/Vegetarian\n",
    "             </div>\n",
    "            '''\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    return m\n",
    "\n",
    "def word2vec_tsne_plot(model, perplexity=40, n_iter=2500):\n",
    "    '''\n",
    "    Creates and TSNE model based on a Gensim word2vec model and plots it, \n",
    "    given parameter inputs of perplexity and number of iterations.\n",
    "    '''\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "\n",
    "    # Reduce 100 dimensional vectors down into 2-dimensional space so that we can see them\n",
    "    tsne_model = TSNE(perplexity=perplexity, n_components=2, init='pca', n_iter=n_iter, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def doc2vec_tsne_plot(doc_model, labels, perplexity=40, n_iter=2500):\n",
    "    '''\n",
    "    Creates and TSNE model based on a Gensim doc2vec model and plots it, \n",
    "    given parameter inputs of perplexity and number of iterations.\n",
    "    '''\n",
    "    tokens = []\n",
    "    for i in range(len(doc_model.docvecs.vectors_docs)):\n",
    "        tokens.append(doc_model.docvecs.vectors_docs[i])\n",
    "\n",
    "    # Reduce 100 dimensional vectors down into 2-dimensional space so that we can see them\n",
    "    tsne_model = TSNE(perplexity=perplexity, n_components=2, init='pca', n_iter=n_iter, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    X = [doc[0] for doc in new_values]\n",
    "    y = [doc[1] for doc in new_values]\n",
    "\n",
    "    # Combine data into DataFrame, so that we plot it easily using Seaborn\n",
    "    df = pd.DataFrame({'X':X, 'y':y, 'Cuisine':labels})\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(x=\"X\", y=\"y\", hue=\"Cuisine\", data=df)\n",
    "    return\n",
    "\n",
    "# Scrape top 100 menus for each cuisine (total of 1000 menus) and save as JSON:\n",
    "# menu_df = scrape_top_menus(['italian','soul-food', 'latin-american', 'gastropub', 'polish', \n",
    "#                            'late-night', 'thai', 'chinese', 'korean', 'vegan_vegetarian'], topn=100)\n",
    "# menu_df.to_json('menu_df_top100.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer our research questions, we need to scrape menu data from restaurants around Chicago. The easiest way to scrape data is to find a central aggregator that uses a common website format (rather than multiple website designs, which would require us to write separate scraping functions for each one). For today's hands-on session, we're going to be scraping menu data from allmenus.com, the menu repository used and updated in the service of GrubHub (it also contains restaurants that do not do takeout, however).\n",
    "\n",
    "Let's first figure out how we can scrape a menu off of a single restaurant's allmenus.com webpage (Francesca's Bryn Mawr, in the Edgewater neighborhood): https://www.allmenus.com/il/chicago/22019-francescas-bryn-mawr/menu/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
